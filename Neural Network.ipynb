{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99278bd9-4a3e-4523-84a7-ae606f88c549",
   "metadata": {},
   "source": [
    "# Neural Network A Simple Perception\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df61e43e-0ea2-48fb-bbe3-ef72af744f07",
   "metadata": {},
   "source": [
    "1. What is deep learning, and how is it connected to artificial intelligence?\n",
    "Deep learning is a subfield of machine learning, which itself is a subset of artificial intelligence (AI).\n",
    "It involves training neural networks with many layers to learn complex patterns from large datasets‚Äîlike images, speech, and text.\n",
    "\n",
    "2. What is a neural network, and what are the different types of neural networks?\n",
    "A neural network is a computational model inspired by the human brain. It consists of layers of neurons (nodes) connected by weights.\n",
    "\n",
    "Types of neural networks:\n",
    "\n",
    "Feedforward Neural Network (FNN) ‚Äì simple, data moves in one direction.\n",
    "\n",
    "Convolutional Neural Network (CNN) ‚Äì for image data.\n",
    "\n",
    "Recurrent Neural Network (RNN) ‚Äì for sequential data (e.g., text).\n",
    "\n",
    "Long Short-Term Memory (LSTM) ‚Äì advanced RNN for long-term dependencies.\n",
    "\n",
    "Transformer ‚Äì for NLP and large language models.\n",
    "\n",
    "Generative Adversarial Network (GAN) ‚Äì for generating data (e.g., images).\n",
    "\n",
    "3. What is the mathematical structure of a neural network?\n",
    "Each neuron performs:\n",
    "       z = w^T x+b\n",
    "\n",
    "   ,  a=ùúô(z)\n",
    "ùë• : input vector\n",
    "\n",
    "ùë§ : weight vector\n",
    "\n",
    "ùëè : bias\n",
    "\n",
    "ùúô : activation function\n",
    "\n",
    "ùëé: output (activation)\n",
    "\n",
    "Layers are stacked to form a network.\n",
    "\n",
    "4. What is an activation function, and why is it essential in a neural network?\n",
    "An activation function introduces non-linearity to the model, enabling the network to learn complex functions. Without it, the network would behave like a simple linear model.\n",
    "\n",
    "5. Common activation functions in neural networks:\n",
    "ReLU: max(0,x)\n",
    "\n",
    "Sigmoid: 1 / ( 1 + e^-x)\n",
    "Tanh:  tanh(x)\n",
    "\n",
    "Leaky ReLU: max(Œ±x,x)\n",
    "\n",
    "Softmax: used in output layer for classification\n",
    "\n",
    "6. What is a multilayer neural network?\n",
    "Also called a deep neural network, it's a network with more than one hidden layer between the input and output layers.\n",
    "\n",
    "7. What is a loss function, and why is it crucial for neural network training?\n",
    "A loss function measures how far the model's predictions are from the true values. It guides the training process by letting the model know how well it's performing.\n",
    "\n",
    "8. Common types of loss functions:\n",
    "Mean Squared Error (MSE) ‚Äì for regression\n",
    "\n",
    "Binary Crossentropy ‚Äì for binary classification\n",
    "\n",
    "Categorical Crossentropy ‚Äì for multi-class classification\n",
    "\n",
    "Huber Loss ‚Äì mix of MSE and MAE\n",
    "\n",
    "9. How does a neural network learn?\n",
    "By forward propagation (to predict output) and backward propagation (to update weights):\n",
    "\n",
    "Predict using input ‚Üí output.\n",
    "\n",
    "Calculate loss.\n",
    "\n",
    "Use gradients to adjust weights (via backpropagation).\n",
    "\n",
    "10. What is an optimizer in neural networks, and why is it necessary?\n",
    "An optimizer updates the model‚Äôs weights to minimize the loss function during training. It decides how fast and in what direction the weights change.\n",
    "\n",
    "11. Common optimizers:\n",
    "SGD (Stochastic Gradient Descent)\n",
    "\n",
    "Adam (adaptive learning rate)\n",
    "\n",
    "RMSprop\n",
    "\n",
    "Adagrad\n",
    "\n",
    "12. What is forward and backward propagation in a neural network?\n",
    "Forward Propagation: Input passes through the layers to get predictions.\n",
    "\n",
    "Backward Propagation: Gradients of loss w.r.t weights are calculated using the chain rule, and weights are updated to reduce error.\n",
    "\n",
    "13. What is weight initialization, and how does it impact training?\n",
    "Initial weights affect:\n",
    "\n",
    "How fast a network trains.\n",
    "\n",
    "Whether it escapes local minima or not.\n",
    "\n",
    "Popular methods:\n",
    "\n",
    "Xavier Initialization\n",
    "\n",
    "He Initialization\n",
    "\n",
    "14. What is the vanishing gradient problem in deep learning?\n",
    "In deep networks, gradients become very small in early layers during backpropagation, making it hard for weights to update‚Äîtraining stalls.\n",
    "\n",
    "15. What is the exploding gradient problem?\n",
    "Gradients become very large, causing unstable updates and often leading to NaNs in training. Happens with poorly initialized weights or deep networks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ffcdc0-00b9-4eb9-8135-79797d26b9dc",
   "metadata": {},
   "source": [
    "# Practical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b1e46b-0e89-4ea0-94ce-c9dd67ddbc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Create a simple perceptron for binary classification:\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(1, activation='sigmoid', input_shape=(n_features,))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0d843e-2b8a-4beb-bce1-536fc5ddcb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##2. Build a neural network with one hidden layer using Keras:\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(16, activation='relu', input_shape=(n_features,)),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd352ce-0016-442b-9289-9cc312f028a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##3. Initialize weights using Xavier (Glorot) initialization in Keras:\n",
    "\n",
    "from tensorflow.keras.initializers import GlorotUniform\n",
    "\n",
    "Dense(16, activation='relu', kernel_initializer=GlorotUniform())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34239657-68f1-4121-8e44-e09a125c7605",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. Apply different activation functions in a Keras model:\n",
    "\n",
    "Dense(32, activation='tanh')\n",
    "Dense(32, activation='relu')\n",
    "Dense(1, activation='sigmoid')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b052017e-4567-499b-9262-5e4e96f95e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5. Add dropout to a model to prevent overfitting:\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(n_features,)),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6136cf91-5b85-4941-9d1f-b0941fe169f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6. Manually implement forward propagation in a simple neural network:\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def forward_propagation(X, weights, bias):\n",
    "    z = np.dot(X, weights) + bias\n",
    "    a = sigmoid(z)\n",
    "    return a\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db3aa27-bd73-4278-b5f2-3f21a011fd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 7. Add batch normalization in Keras:\n",
    "\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(n_features,)),\n",
    "    BatchNormalization(),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8a20bc-2f5f-4e84-98c1-b9d9bedf3b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 8. Visualize training process with accuracy and loss curves:\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10)\n",
    "\n",
    "plt.plot(history.history['loss'], label='Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.plot(history.history['accuracy'], label='Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc434f87-f7cd-4972-883a-bfe6a138c8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 9. Use gradient clipping in Keras:\n",
    "\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "optimizer = Adam(clipvalue=1.0)  # or clipnorm=1.0\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f77861-3969-4dc0-9862-4f2607335eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 10. Create a custom loss function in Keras:\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "\n",
    "model.compile(optimizer='adam', loss=custom_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd7f897-e8f1-48b1-8264-d26b1f7f8020",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 11. Visualize the structure of a neural network model in Keras:\n",
    "\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
